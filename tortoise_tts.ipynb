{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edrihan/tortoise-tts/blob/main/tortoise_tts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font face=\"Trebuchet MS\" size=\"6\">Tortoise TTS<font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><font color=\"#999\" size=\"4\">Text to spoken word audio</font><font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><a href=\"https://github.com/olaviinha/NeuralTextToAudio\" target=\"_blank\"><font color=\"#999\" size=\"4\">Github</font></a>\n",
        "\n",
        "- All file and directory paths should be relative to your Google Drive root (_My Drive_). E.g. `voice_audio` value should be `Audio/test-voice.wav`, if you have a directory called _Audio_ in your drive, and you want to use _test-voice.wav_ from that directory. Paths are case-sensitive.\n",
        "- This notebook will attempt to prepare a coherent voice dataset from `voice_audio` input, but optimal `voice_audio` for coherent output should be a path to a WAV file of about 1 minute in duration, or a directory containing a total of about 1 minute of WAV files.\n",
        "- In case `voice_audio` contents exceeds 1 minute considerably, random clips (from random file, or files depending on contents, if directory given) will be picked for voice cloning."
      ],
      "metadata": {
        "id": "GBgr33OisX3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Setup\n",
        "#@markdown This cell needs to be run only once. It will mount your Google Drive and setup prerequisites.<br>\n",
        "#@markdown <small>Mounting Drive will enable this notebook to save outputs directly to your Drive. Otherwise you will need to copy/download them manually from this notebook.</small>\n",
        "\n",
        "force_setup = False\n",
        "repositories = ['https://github.com/neonbjb/tortoise-tts.git']\n",
        "pip_packages = 'scipy transformers==4.19.0'\n",
        "apt_packages = 'sox'\n",
        "mount_drive = False #@param {type:\"boolean\"}\n",
        "skip_setup = False #@ param {type:\"boolean\"}\n",
        "\n",
        "# Download the repo from Github\n",
        "import os\n",
        "from google.colab import output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%cd /content/\n",
        "\n",
        "# inhagcutils\n",
        "if not os.path.isfile('/content/inhagcutils.ipynb') and force_setup == False:\n",
        "  !pip -q install import-ipynb {pip_packages}\n",
        "  if apt_packages != '':\n",
        "    !apt-get update && apt-get install {apt_packages}\n",
        "  !curl -s -O https://raw.githubusercontent.com/olaviinha/inhagcutils/master/inhagcutils.ipynb\n",
        "import import_ipynb\n",
        "from inhagcutils import *\n",
        "\n",
        "# Mount Drive\n",
        "if mount_drive == True:\n",
        "  if not os.path.isdir('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    drive_root = '/content/drive/My Drive'\n",
        "  if not os.path.isdir('/content/mydrive'):\n",
        "    os.symlink('/content/drive/My Drive', '/content/mydrive')\n",
        "    drive_root = '/content/mydrive/'\n",
        "  drive_root_set = True\n",
        "else:\n",
        "  create_dirs(['/content/faux_drive'])\n",
        "  drive_root = '/content/faux_drive/'\n",
        "\n",
        "if len(repositories) > 0 and skip_setup == False:\n",
        "  for repo in repositories:\n",
        "    %cd /content/\n",
        "    install_dir = fix_path('/content/'+path_leaf(repo).replace('.git', ''))\n",
        "    repo = repo if '.git' in repo else repo+'.git'\n",
        "    !git clone {repo}\n",
        "    if os.path.isfile(install_dir+'requirements.txt'):\n",
        "      !pip install -r {install_dir}/requirements.txt\n",
        "    if os.path.isfile(install_dir+'setup.py') or os.path.isfile(install_dir+'setup.cfg'):\n",
        "      !pip install -e {install_dir}\n",
        "\n",
        "if len(repositories) == 1:\n",
        "  %cd {install_dir}\n",
        "\n",
        "dir_tmp = '/content/tmp/'\n",
        "dir_tmp_corpus = '/content/tmp/corpus/'\n",
        "dir_tmp_slices = '/content/tmp/slices/'\n",
        "dir_tmp_clips = '/content/tmp/clips/'\n",
        "dir_tmp_processed = '/content/tmp/processed/'\n",
        "create_dirs([dir_tmp, dir_tmp_corpus, dir_tmp_slices, dir_tmp_clips, dir_tmp_processed])\n",
        "\n",
        "import time, sys\n",
        "from datetime import timedelta\n",
        "import math\n",
        "\n",
        "# Imports used through the rest of the notebook.\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import IPython\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "from tortoise.api import TextToSpeech\n",
        "from tortoise.utils.audio import load_audio, load_voice, load_voices\n",
        "\n",
        "def slice_to_frames(audio_data, slice_duration, fade_in=0, fade_out=0, sr=44100):\n",
        "  a_duration = librosa.get_duration(audio_data, sr=sr)\n",
        "  clips = math.ceil(a_duration/slice_duration)\n",
        "  frames = []\n",
        "  for i in range(clips-1):\n",
        "    if i > 0 and i < clips:\n",
        "      start = i*slice_duration\n",
        "      audio_clip = clip_audio(audio_data, start, slice_duration)\n",
        "      if fade_in > 0 or fade_out > 0:\n",
        "        audio_clip = fade_audio(audio_clip, fade_in, fade_out, sr=sr)\n",
        "      frames.append(audio_clip)\n",
        "  return frames\n",
        "\n",
        "def clip_audio(audio_data, start, duration, sr=44100):\n",
        "  xstart = librosa.time_to_samples(start, sr=sr)\n",
        "  xduration = librosa.time_to_samples(start+duration, sr=sr)\n",
        "  audio_data = audio_data[:, xstart:xduration]\n",
        "  return audio_data\n",
        "\n",
        "def fade_audio(audio_data, fade_in=0.05, fade_out=0.05, sr=44100):\n",
        "  a_duration = librosa.get_duration(audio_data, sr=sr)\n",
        "  if fade_in > 0:\n",
        "    fade_in_to = librosa.time_to_samples(fade_in, sr=sr)\n",
        "    in_y = audio_data[:, 0:fade_in_to]\n",
        "    fade_ins = []\n",
        "    for channel in in_y:\n",
        "      fade = [ i/len(channel)*smp for i, smp in enumerate(channel) ]\n",
        "      fade_ins.append(fade)\n",
        "    fade_ins = np.array(fade_ins)\n",
        "    tail_start = fade_in_to+1\n",
        "    tail = audio_data[:, tail_start:]\n",
        "    audio_data = np.concatenate([fade_ins, tail], axis=1)\n",
        "  if fade_out > 0:\n",
        "    fade_out_start = librosa.time_to_samples(a_duration-fade_out, sr=sr)\n",
        "    out_y = audio_data[:, fade_out_start:]\n",
        "    fade_outs = []\n",
        "    for channel in out_y:\n",
        "      fade = [ smp-(i/len(channel)*smp) for i, smp in enumerate(channel) ]\n",
        "      fade_outs.append(fade)\n",
        "    fade_outs = np.array(fade_outs)\n",
        "    head_start = fade_out_start-1\n",
        "    head = audio_data[:, :head_start]\n",
        "    audio_data = np.concatenate([head, fade_outs], axis=1)\n",
        "  return audio_data\n",
        "\n",
        "def remove_silence(audio, window_size=0.2, threshold=0.1, save_as='', sr=44100):\n",
        "  if type(audio) != np.ndarray:\n",
        "    y, sr = librosa.load(audio, sr=None, mono=False)\n",
        "  else:\n",
        "    y = audio\n",
        "  audio_slices = slice_to_frames(y, window_size, sr=sr)\n",
        "  silence_removed_list = []\n",
        "  for audio_slice in audio_slices:\n",
        "    if max(audio_slice[0]) > threshold or max(audio_slice[1]) < -abs(threshold):\n",
        "      silence_removed_list.append(audio_slice)\n",
        "  silence_removed = np.concatenate(silence_removed_list, axis=1)\n",
        "  if save_as != '':\n",
        "    sf.write(save_as, silence_removed.T, sr)\n",
        "    return save_as\n",
        "  return silence_removed\n",
        "\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "def get_audio_duration(file):\n",
        "  y, sr = librosa.load(voice_file, sr=None, mono=True)\n",
        "  return librosa.get_duration(y, sr=sr)\n",
        "\n",
        "def get_dir_size(dir_path='.'):\n",
        "  total_size = 0\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    for f in filenames:\n",
        "      fp = os.path.join(dirpath, f)\n",
        "      if not os.path.islink(fp):\n",
        "        total_size += os.path.getsize(fp)\n",
        "  return total_size\n",
        "\n",
        "def chop_to_sentences(text):\n",
        "  delimiter = '.'\n",
        "  temp = [e+delimiter for e in text.split(delimiter) if e]\n",
        "  sentences = []\n",
        "  for sentence in temp:\n",
        "    delimiter = '?'\n",
        "    if delimiter in sentence:\n",
        "      wtf = sentence.split(delimiter)\n",
        "      for f in wtf:\n",
        "        if f[-1] != '.' and f[-1] != '?' and f[-1] != '?':\n",
        "          f = f+'?'\n",
        "        if f != '':\n",
        "          sentences.append(f.strip())\n",
        "    elif sentence.strip() != '' and len(sentence.strip()) > 1:\n",
        "      sentences.append(sentence.strip())\n",
        "  return sentences\n",
        "\n",
        "# This will download all the models used by Tortoise from the HuggingFace hub.\n",
        "tts = TextToSpeech()\n",
        "\n",
        "output.clear()\n",
        "# !nvidia-smi\n",
        "op(c.ok, 'Setup finished.', time=True)"
      ],
      "metadata": {
        "id": "Zl44n6FXsbnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9be3c9a-b908-4233-8705-5dbbdcf15138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2023-08-30 05:53:41 \u001b[92mSetup finished.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "46OP_UIHEs76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import zipfile\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "folder_path = \"/content/faux_drive/fullset/\"\n",
        "zip_file = zipfile.ZipFile(\"/content/faux_drive/fullset/fimdom.zip\", 'w')\n",
        "for root, dirs, filez in os.walk(folder_path):\n",
        "    for file in tqdm(filez,desc=\"zipping files\"):\n",
        "        file_path = os.path.join(root, file)\n",
        "        extension = file_path.split('.')[-1]\n",
        "        files.download(file_path)\n",
        "        if extension in ('wav','mp3','flac'):\n",
        "\n",
        "          # Specify the paths for the input WAV file and output FLAC file\n",
        "          input_file = file_path\n",
        "          output_flac = file_path.replace('.'+extension,'.flac')\n",
        "          # Read the WAV file\n",
        "          data, samplerate = sf.read(input_file)\n",
        "\n",
        "          # Write the audio data to a FLAC file\n",
        "          sf.write(output_flac, data, samplerate, format='flac')\n",
        "          zip_file.write(output_flac, os.path.relpath(file_path, folder_path))\n",
        "          !rm -rf {file_path}\n",
        "        else:\n",
        "          zip_file.write(file_path, os.path.relpath(file_path, folder_path))\n",
        "          !rm -rf {file_path}\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "0UWYqXSbgROA",
        "outputId": "d6c64ea7-0c7a-434f-8a52-58ac19ce2f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport zipfile\\nimport soundfile as sf\\nfrom tqdm import tqdm\\nfrom google.colab import files\\nfolder_path = \"/content/faux_drive/fullset/\"\\nzip_file = zipfile.ZipFile(\"/content/faux_drive/fullset/fimdom.zip\", \\'w\\')\\nfor root, dirs, filez in os.walk(folder_path):\\n    for file in tqdm(filez,desc=\"zipping files\"):\\n        file_path = os.path.join(root, file)\\n        extension = file_path.split(\\'.\\')[-1]\\n        files.download(file_path)\\n        if extension in (\\'wav\\',\\'mp3\\',\\'flac\\'):\\n\\n          # Specify the paths for the input WAV file and output FLAC file\\n          input_file = file_path\\n          output_flac = file_path.replace(\\'.\\'+extension,\\'.flac\\')\\n          # Read the WAV file\\n          data, samplerate = sf.read(input_file)\\n\\n          # Write the audio data to a FLAC file\\n          sf.write(output_flac, data, samplerate, format=\\'flac\\')\\n          zip_file.write(output_flac, os.path.relpath(file_path, folder_path))\\n          !rm -rf {file_path}\\n        else:\\n          zip_file.write(file_path, os.path.relpath(file_path, folder_path))\\n          !rm -rf {file_path}\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Audio tool installation\n",
        "#!pip install --upgrade youtube-dl\n",
        "!pip uninstall -y youtube_dl\n",
        "!pip install git+https://github.com/ytdl-org/youtube-dl.git@master#egg=youtube_dl\n",
        "\n",
        "!pip install sox\n",
        "!pip install pydub\n",
        "from pydub import AudioSegment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go-6bUVpwzXf",
        "outputId": "72e9d21c-f2f0-4039-9db2-60fd8e707fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: youtube-dl 2021.12.17\n",
            "Uninstalling youtube-dl-2021.12.17:\n",
            "  Successfully uninstalled youtube-dl-2021.12.17\n",
            "Collecting youtube_dl\n",
            "  Cloning https://github.com/ytdl-org/youtube-dl.git (to revision master) to /tmp/pip-install-shb0cssa/youtube-dl_0860a14b54c3420fbc63c271b9c67cc2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ytdl-org/youtube-dl.git /tmp/pip-install-shb0cssa/youtube-dl_0860a14b54c3420fbc63c271b9c67cc2\n",
            "  Resolved https://github.com/ytdl-org/youtube-dl.git to commit 86e3cf5e5849aefcc540c19bb5fa5ab7f470d1c1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: youtube_dl\n",
            "  Building wheel for youtube_dl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for youtube_dl: filename=youtube_dl-2021.12.17-py2.py3-none-any.whl size=1939858 sha256=78ba30d0990410eb12a625cdb2f0f2fd32c19e55a97143eaa6b92e85abb92f4a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ssm8zjza/wheels/b8/03/62/9c414b89a26da510b0a6d984b0ba74200d591e3d0abfa72aa8\n",
            "Successfully built youtube_dl\n",
            "Installing collected packages: youtube_dl\n",
            "Successfully installed youtube_dl-2021.12.17\n",
            "Requirement already satisfied: sox in /usr/local/lib/python3.10/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from sox) (1.23.5)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Download audio and process\n",
        "voices_settings = {\n",
        "    \"sarl\": [\n",
        "        {\n",
        "            'link': 'https://www.youtube.com/watch?v=wupToqz1e2g',\n",
        "            'start_sec': 0.21,\n",
        "            'end_sec': 9.29\n",
        "         },\n",
        "        {\n",
        "            'link':'https://www.youtube.com/watch?v=nGanLUnjoPI',\n",
        "            'start_sec': 60.56,\n",
        "            'end_sec': 75.56,\n",
        "        },\n",
        "         #       {\n",
        "        #    'link':'https://www.youtube.com/watch?v=UnURElCzGc0',\n",
        "        #    'start_sec': 3.372,\n",
        "        #    'end_sec': 17,\n",
        "        #},\n",
        "        #{\n",
        "        #    'link':'https://www.youtube.com/watch?v=UnURElCzGc0',\n",
        "        #    'start_sec': 3 * 60 + 26,\n",
        "        #    'end_sec': 3 * 60 + 42,\n",
        "        #},\n",
        "       #         {\n",
        "       #     'link':'https://www.loc.gov/item/cosmos000110/',\n",
        "       #     'start_sec': 0,\n",
        "      #     'end_sec': 20,\n",
        "      #  },\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ]\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "for voice, refs in voices_settings.items():\n",
        "  for r, ref in enumerate(refs):\n",
        "\n",
        "    #voice_path = '/content/tortoise-tts/tortoise/voices'\n",
        "    voices_path = os.path.join(drive_root,\"voices\")\n",
        "    voices_path = os.path.join(drive_root)\n",
        "    voice_path = os.path.join(voices_path,voice)\n",
        "    filename =  voice + '.mp4'\n",
        "    filepath = os.path.join(voice_path,filename)\n",
        "    chunkpath =  os.path.join(voice_path,str(r) + '.wav')\n",
        "    inputs_dir = os.path.join(drive_root,'voice_inputs')\n",
        "    input_name = ref[\"link\"].split(\"=\")[-1]\n",
        "    input_path = os.path.join(inputs_dir,input_name + '.wav')\n",
        "    if not os.path.isfile(input_path):\n",
        "      print(f'downloading {ref[\"link\"]} because {input_path} is not a file')\n",
        "      #command = f'mkdir {voices_path}; cd {voices_path} ; mkdir {voice} ; cd {voice_path};' + \\\n",
        "      #          f'youtube-dl -x --audio-format wav {ref[\"link\"]} --output \"{str(r)+\"_complete\"}.%(ext)s\"'\n",
        "\n",
        "      !mkdir {inputs_dir}\n",
        "\n",
        "      command = f\"cd {inputs_dir} && \" + \\\n",
        "                f'youtube-dl -x --audio-format wav {ref[\"link\"]} --output \"{input_name}.%(ext)s\"'\n",
        "\n",
        "      print('running command:')\n",
        "      print(command)\n",
        "      !{command}\n",
        "    else:\n",
        "      print(f'skipped downloading {ref[\"link\"]} because {input_path} exists')\n",
        "\n",
        "\n",
        "    !mkdir {os.path.dirname(chunkpath)}\n",
        "\n",
        "    #Trim\n",
        "    !rm -rf {chunkpath}\n",
        "    command = f\"sox {input_path} {chunkpath} trim {ref['start_sec']} {ref['end_sec'] - ref['start_sec']}\"\n",
        "    print('running command:')\n",
        "    print(command)\n",
        "    !{command}\n",
        "\n",
        "    #Normalize\n",
        "\n",
        "    audio = AudioSegment.from_file(chunkpath)\n",
        "    normalized_audio = audio.normalize()\n",
        "    compressed_audio = normalized_audio.compress_dynamic_range(threshold=-5,ratio=2,attack=5,release=50)\n",
        "    renormalized_audio = compressed_audio.normalize()\n",
        "    renormalized_audio.export(chunkpath, format=\"wav\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#!cd /content/faux_drive && youtube-dl --extract-audio --audio-format wav https://lotushelix.bandcamp.com/track/stranger-in-the-street --output \"stranger.wav\"\n",
        "#!cd /content/faux_drive && youtube-dl --extract-audio --audio-format wav https://lotushelix.bandcamp.com/track/live-life-in-love --output \"love.wav\"\n",
        "#!cd /content/faux_drive && youtube-dl --extract-audio --audio-format wav https://youtu.be/3u3JSEqNtlg --output \"technique.wav\"\n",
        "#!cd /content/faux_drive && youtube-dl --extract-audio --audio-format wav https://www.youtube.com/watch?v=wupToqz1e2g --output \"sagan.wav\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McYb8Om5xJF3",
        "outputId": "fd11281b-e8ba-4207-876e-26b5c1e601dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipped downloading https://www.youtube.com/watch?v=wupToqz1e2g because /content/faux_drive/voice_inputs/wupToqz1e2g.wav exists\n",
            "mkdir: cannot create directory ‘/content/faux_drive/sarl’: File exists\n",
            "running command:\n",
            "sox /content/faux_drive/voice_inputs/wupToqz1e2g.wav /content/faux_drive/sarl/0.wav trim 0.21 9.079999999999998\n",
            "skipped downloading https://www.youtube.com/watch?v=nGanLUnjoPI because /content/faux_drive/voice_inputs/nGanLUnjoPI.wav exists\n",
            "mkdir: cannot create directory ‘/content/faux_drive/sarl’: File exists\n",
            "running command:\n",
            "sox /content/faux_drive/voice_inputs/nGanLUnjoPI.wav /content/faux_drive/sarl/1.wav trim 60.56 15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import time\n",
        "from datetime import datetime\n",
        "#@title # Generate spoken word audio\n",
        "text = \"bishop ay 1. bishop ay 2. bishop ay 3. bishop ay 4. bishop ay 5. bishop ay 6. bishop ay 7. bishop ay 8. bishop bee 1. bishop bee 2. bishop bee 3. bishop bee 4. bishop bee 5. bishop bee 6. bishop bee 7. bishop bee 8. bishop sea 1. bishop sea 2. bishop sea 3. bishop sea 4. bishop sea 5. bishop sea 6. bishop sea 7. bishop sea 8. bishop dee 1. bishop dee 2. bishop dee 3. bishop dee 4. bishop dee 5. bishop dee 6. bishop dee 7. bishop dee 8. bishop ee 1. bishop ee 2. bishop ee 3. bishop ee 4. bishop ee 5. bishop ee 6. bishop ee 7. bishop ee 8. bishop eff 1. bishop eff 2. bishop eff 3. bishop eff 4. bishop eff 5. bishop eff 6. bishop eff 7. bishop eff 8. bishop jee 1. bishop jee 2. bishop jee 3. bishop jee 4. bishop jee 5. bishop jee 6. bishop jee 7. bishop jee 8. bishop aych 1. bishop aych 2. bishop aych 3. bishop aych 4. bishop aych 5. bishop aych 6. bishop aych 7. bishop aych 8. king ay 1. king ay 2. king ay 3. king ay 4. king ay 5. king ay 6. king ay 7. king ay 8. king bee 1. king bee 2. king bee 3. king bee 4. king bee 5. king bee 6. king bee 7. king bee 8. king sea 1. king sea 2. king sea 3. king sea 4. king sea 5. king sea 6. king sea 7. king sea 8. king dee 1. king dee 2. king dee 3. king dee 4. king dee 5. king dee 6. king dee 7. king dee 8. king ee 1. king ee 2. king ee 3. king ee 4. king ee 5. king ee 6. king ee 7. king ee 8. king eff 1. king eff 2. king eff 3. king eff 4. king eff 5. king eff 6. king eff 7. king eff 8. king jee 1. king jee 2. king jee 3. king jee 4. king jee 5. king jee 6. king jee 7. king jee 8. king aych 1. king aych 2. king aych 3. king aych 4. king aych 5. king aych 6. king aych 7. king aych 8. queen ay 1. queen ay 2. queen ay 3. queen ay 4. queen ay 5. queen ay 6. queen ay 7. queen ay 8. queen bee 1. queen bee 2. queen bee 3. queen bee 4. queen bee 5. queen bee 6. queen bee 7. queen bee 8. queen sea 1. queen sea 2. queen sea 3. queen sea 4. queen sea 5. queen sea 6. queen sea 7. queen sea 8. queen dee 1. queen dee 2. queen dee 3. queen dee 4. queen dee 5. queen dee 6. queen dee 7. queen dee 8. queen ee 1. queen ee 2. queen ee 3. queen ee 4. queen ee 5. queen ee 6. queen ee 7. queen ee 8. queen eff 1. queen eff 2. queen eff 3. queen eff 4. queen eff 5. queen eff 6. queen eff 7. queen eff 8. queen jee 1. queen jee 2. queen jee 3. queen jee 4. queen jee 5. queen jee 6. queen jee 7. queen jee 8. queen aych 1. queen aych 2. queen aych 3. queen aych 4. queen aych 5. queen aych 6. queen aych 7. queen aych 8\" #@param {type:\"string\"}\n",
        "voice_audio = \"sarl\" #@param {type:\"string\"}\n",
        "combo_voice = False #@param {type:\"boolean\"}\n",
        "preset = \"ultra_fast\" #@param [\"standard\", \"fast\", \"ultra_fast\", \"high_quality\"]\n",
        "output_dir = \"fullset\" #@param {type:\"string\"}\n",
        "end_session_when_done = False #@param {type: \"boolean\"}\n",
        "save_sentences_as_they_render = False #@param {type: \"boolean\"}\n",
        "zip_sentences_and_download_all = True #@param {type: \"boolean\"}\n",
        "# @markdown `download_partial_zip_after_minutes <= 0` will disable this behavior\n",
        "download_partial_zip_after_minutes = 4 #@param {type:\"number\"}\n",
        "if download_partial_zip_after_minutes < 0:\n",
        "  download_partial_zip_after_minutes = 0\n",
        "\n",
        "save_txt = True\n",
        "timer_start = time.time()\n",
        "try:\n",
        "  uniq_id = gen_id()\n",
        "except:\n",
        "  raise Exception('Restart and run all')\n",
        "\n",
        "folder_path = \"/content/faux_drive/fullset/\"\n",
        "slice_length = 12 # seconds per slice\n",
        "use_slices = 5 # slices to use\n",
        "optimal_samples_duration = slice_length * use_slices # total duration\n",
        "sample_rate = 24000\n",
        "#process this many sentences in one go\n",
        "# @markdown try lowering `chunk_sentences` if you run out of VRAM. It worked with 8 with a high-RAM environment:\n",
        "chunk_sentences = 1 #@param {type:\"integer\", description:\"If you run out of (v)RAM try lowering this\"}\n",
        "dir_byte_limit = 48000000\n",
        "merge_sentences = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "start_time = datetime.now()\n",
        "if zip_sentences_and_download_all:\n",
        "  zip_name = uniq_id\n",
        "  zip_file = zipfile.ZipFile(f'{zip_name}.zip','w')\n",
        "\n",
        "\n",
        "if download_partial_zip_after_minutes:\n",
        "\n",
        "  partial_zip_num = 0\n",
        "  partial_zip_start_time = datetime.now()\n",
        "  partial_zip_name = f'{uniq_id}_{partial_zip_num}'\n",
        "  partial_zip_path = os.path.join(folder_path,\n",
        "                                partial_zip_name + '.zip')\n",
        "  partial_zip_file = zipfile.ZipFile(f'{partial_zip_path}','w',)\n",
        "  def close_zip_part():\n",
        "\n",
        "    global partial_zip_file, partial_zip_path\n",
        "    print(f\"closing {partial_zip_path}\")\n",
        "    # Finsh last one\n",
        "    partial_zip_file.close()\n",
        "    print(f'sleeping for {10}s')\n",
        "    time.sleep(10)\n",
        "    print(f\"trying to start download of {partial_zip_path}\")\n",
        "\n",
        "    files.download(partial_zip_path)\n",
        "    !wget -q http://www.yoursite.com/file.csv\n",
        "\n",
        "\n",
        "\n",
        "  def new_zip_part():\n",
        "    global partial_zip_num\n",
        "    global partial_zip_start_time, partial_zip_name\n",
        "    global partial_zip_path, partial_zip_file\n",
        "      # INIT partial_zip\n",
        "    close_zip_part()\n",
        "    #start next one\n",
        "    partial_zip_num += 1\n",
        "    partial_zip_start_time = datetime.now()\n",
        "    partial_zip_name = f'{uniq_id}_{partial_zip_num}'\n",
        "    partial_zip_path = os.path.join(folder_path,\n",
        "                                    partial_zip_name + '.zip')\n",
        "    partial_zip_file = zipfile.ZipFile(f'{partial_zip_path}','w',)\n",
        "    print('created {partial_zip_path}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "op(c.title, 'Run ID:', uniq_id, time=True)\n",
        "print()\n",
        "\n",
        "voice_corpus = voice_audio\n",
        "prompts = chop_to_sentences(text)\n",
        "\n",
        "if chunk_sentences > 1:\n",
        "  prompts = [''.join(prompts[i:i+chunk_sentences]) for i in range(0, len(prompts), chunk_sentences)]\n",
        "\n",
        "clean_dirs([dir_tmp_corpus, dir_tmp_slices, dir_tmp_clips, dir_tmp_processed])\n",
        "\n",
        "if os.path.isfile(drive_root+voice_corpus):\n",
        "  clean_dirs([dir_tmp_corpus])\n",
        "  shutil.copy(drive_root+voice_corpus, dir_tmp_corpus)\n",
        "  voice_dirs = [dir_tmp_corpus]\n",
        "else:\n",
        "  if voice_corpus == 'voice_list':\n",
        "    voice_dirs = [drive_root+x for x in voice_list]\n",
        "  elif ',' in voice_corpus:\n",
        "    voice_dirs = [drive_root+fix_path(x.strip()) for x in voice_corpus.split(',')]\n",
        "  elif ';' in voice_corpus:\n",
        "    voice_dirs = [drive_root+fix_path(x.strip()) for x in voice_corpus.split(';')]\n",
        "  else:\n",
        "    voice_dirs = [drive_root+fix_path(voice_corpus)]\n",
        "\n",
        "# Output\n",
        "if output_dir == '':\n",
        "  if mount_drive == True:\n",
        "    dir_out = dir_tmp\n",
        "  else:\n",
        "    dir_out = drive_root\n",
        "else:\n",
        "  if not os.path.isdir(drive_root+output_dir):\n",
        "    os.mkdir(drive_root+output_dir)\n",
        "  dir_out = drive_root+fix_path(output_dir)\n",
        "\n",
        "total = len(voice_dirs * len(prompts))\n",
        "use_voices = []\n",
        "\n",
        "txt_file = dir_out+uniq_id+'.txt'\n",
        "if save_txt: append_txt(txt_file, timestamp(human_readable=True)+' '+uniq_id+'\\n\\n'+text+'\\n\\n'+'combo_voice: '+str(combo_voice)+'\\n'+'preset: '+preset+'\\n'+'dir_out: '+dir_out+'\\n\\n')\n",
        "\n",
        "for i, voice_dir in enumerate(voice_dirs, 1):\n",
        "  if voice_dir == dir_tmp_corpus:\n",
        "    voice_name = basename(voice_corpus)\n",
        "  else:\n",
        "    voice_name = path_leaf(voice_dir)\n",
        "\n",
        "  use_voices.append(voice_name)\n",
        "  new_voice_dir = '/content/tortoise-tts/tortoise/voices/'+voice_name+'/'\n",
        "  if not os.path.isdir(new_voice_dir):\n",
        "    os.mkdir(new_voice_dir)\n",
        "  else:\n",
        "    clean_dirs([new_voice_dir])\n",
        "  voice_files = list_audio(voice_dir)\n",
        "\n",
        "  random.shuffle(voice_files)\n",
        "\n",
        "  if save_txt: append_txt(txt_file, voice_name+'\\n'+'In: '+voice_dir)\n",
        "\n",
        "  if len(voice_files) == 0:\n",
        "    print()\n",
        "    op(c.fail, 'Skipping '+voice_name+' - Reason: WAV files not found in dir:', voice_dir.replace(drive_root, ''), time=True)\n",
        "    if save_txt: append_txt(txt_file, 'Out: - (no wav found, SKIP)\\n')\n",
        "  else:\n",
        "    op(c.okb, 'Processing voice files...', time=True)\n",
        "    bytes_collected = 0\n",
        "    for voice_file in voice_files:\n",
        "      voice_file = remove_silence(voice_file, window_size=2, threshold=0.1, save_as=dir_tmp_processed+path_leaf(voice_file))\n",
        "      file_duration = get_audio_duration(voice_file)\n",
        "      slice_file = dir_tmp_slices+path_leaf(voice_file)\n",
        "\n",
        "      if file_duration > slice_length:\n",
        "        !sox {sox_q} \"{voice_file}\" -r 22050 {slice_file} trim 0 {slice_length} : newfile : restart\n",
        "      else:\n",
        "        !sox {sox_q} \"{voice_file}\" -r 22050 {slice_file}\n",
        "\n",
        "      clips = list_audio(dir_tmp_slices)\n",
        "\n",
        "      short_clips = []\n",
        "      long_clips = []\n",
        "      for clip in clips:\n",
        "        clip_duration = get_audio_duration(clip)\n",
        "        if clip_duration >= slice_length:\n",
        "          long_clips.append(clip)\n",
        "        else:\n",
        "          short_clips.append(clip)\n",
        "        if (len(long_clips)*slice_length >= optimal_samples_duration):\n",
        "          break\n",
        "\n",
        "      if len(long_clips) >= use_slices:\n",
        "        selected_clips = random.sample(long_clips, use_slices)\n",
        "      else:\n",
        "        selected_clips = clips\n",
        "\n",
        "      if save_txt: append_txt(txt_file, 'Selected clips:')\n",
        "      for clip in selected_clips:\n",
        "        if save_txt: append_txt(txt_file, path_leaf(clip)+'\\n')\n",
        "        shutil.copy(clip, new_voice_dir)\n",
        "\n",
        "      file_size = os.path.getsize(voice_file)\n",
        "      bytes_collected += file_size\n",
        "      if bytes_collected > dir_byte_limit:\n",
        "        break\n",
        "\n",
        "    merge_list = []\n",
        "    for ii, text in enumerate(prompts, 1):\n",
        "\n",
        "      ndx_info = str(i*ii)+'/'+str(total)+' '\n",
        "\n",
        "      voice_samples = None\n",
        "      conditioning_latents = None\n",
        "      gen = None\n",
        "\n",
        "      print()\n",
        "      op(c.title, ndx_info+'Processing', voice_name, time=True)\n",
        "\n",
        "      if combo_voice == False:\n",
        "        op(c.title, ndx_info+'Synthesizing', text+'...', time=True)\n",
        "\n",
        "        file_out = dir_out+uniq_id+'__'+voice_name+'_'+str(ii).zfill(3)+'_'+slug(text[:60])+'.wav'\n",
        "        if save_txt: append_txt(txt_file, 'Out: '+file_out+'\\n')\n",
        "        voice_samples, conditioning_latents = load_voice(voice_name)\n",
        "        gen = tts.tts_with_preset(text, voice_samples=voice_samples, conditioning_latents=conditioning_latents, preset=preset)\n",
        "        torchaudio.save(file_out, gen.squeeze(0).cpu(), sample_rate)\n",
        "        if os.path.isfile(file_out):\n",
        "          op(c.ok, 'Saved', file_out.replace(drive_root, ''), time=True)\n",
        "          merge_list.append(file_out)\n",
        "          if save_sentences_as_they_render:\n",
        "            files.download(file_out)\n",
        "          if zip_sentences_and_download_all:\n",
        "            print(f'adding {file_out} to {zip_name}.zip')\n",
        "            zip_file.write(file_out,\n",
        "              os.path.relpath(partial_zip_path, folder_path))\n",
        "          if bool(download_partial_zip_after_minutes):\n",
        "            print(f'adding {file_out} to {partial_zip_file}.zip')\n",
        "            partial_zip_file.write(file_out,\n",
        "              os.path.relpath(partial_zip_path, folder_path)) #HERE\n",
        "            elapsed = datetime.now() -partial_zip_start_time\n",
        "            minutes = elapsed.total_seconds() / 60\n",
        "            if ii == (len(prompts) - 1):\n",
        "              close_zip_part()\n",
        "            elif minutes > download_partial_zip_after_minutes:\n",
        "              new_zip_part()\n",
        "\n",
        "        else:\n",
        "          op(c.fail, 'Error saving', file_out.replace(drive_root, ''), time=True)\n",
        "\n",
        "        del voice_samples\n",
        "        del conditioning_latents\n",
        "        del gen\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "      import gc\n",
        "      gc.collect()\n",
        "\n",
        "    if merge_sentences == True:\n",
        "      sox_input_list = ' '.join(merge_list)\n",
        "      sox_merge_out = dir_out+uniq_id+'__'+voice_name+'_FULL.wav'\n",
        "      !sox {sox_q} {sox_input_list} {sox_merge_out}\n",
        "\n",
        "if combo_voice == True:\n",
        "   for text in prompts:\n",
        "     print()\n",
        "     op(c.title, 'Synthesizing', text[:40]+'...', time=True)\n",
        "     file_out = dir_out+uniq_id+'__'+voice_name+'_'+slug(text[:60])+'.wav'\n",
        "     if save_txt == True:\n",
        "       append_txt(txt_file, 'Out: '+file_out+'\\n')\n",
        "     voice_samples, conditioning_latents = load_voices(use_voices)\n",
        "     gen = tts.tts_with_preset(text, voice_samples=voice_samples, conditioning_latents=conditioning_latents, preset=preset)\n",
        "     torchaudio.save(file_out, gen.squeeze(0).cpu(), sample_rate)\n",
        "     if save_sentences_as_they_render:\n",
        "      files.download(file_out)\n",
        "\n",
        "     # IPython.display.Audio(file_out)\n",
        "\n",
        "     del voice_samples\n",
        "     del conditioning_latents\n",
        "     del gen\n",
        "     del tts\n",
        "     torch.cuda.empty_cache()\n",
        "     import gc\n",
        "     gc.collect()\n",
        "\n",
        "\n",
        "timer_end = time.time()\n",
        "\n",
        "print()\n",
        "\n",
        "if save_txt: append_txt(txt_file, str(timedelta(seconds=timer_end-timer_start)) )\n",
        "if save_txt: append_txt(txt_file, 'Finished at '+timestamp(human_readable=True))\n",
        "if zip_sentences_and_download_all:\n",
        "  zip_file.write(txt_file)\n",
        "files.download(f'{zip_name}.zip')\n",
        "\n",
        "op(c.okb, 'Elapsed', timedelta(seconds=timer_end-timer_start), time=True)\n",
        "op(c.ok, 'FIN.')\n",
        "\n",
        "if end_session_when_done is True: end_session()"
      ],
      "metadata": {
        "id": "DjMTvst0z2ng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "outputId": "ed216d87-c060-42ac-9510-d76751a3b9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2023-08-30 06:05:04 \u001b[96mRun ID:\u001b[0m pyrxut\n",
            "\n",
            "\u001b[90m2023-08-30 06:05:04 \u001b[94mProcessing voice files...\u001b[0m\n",
            "\n",
            "\u001b[90m2023-08-30 06:05:04 \u001b[96m1/192 Processing\u001b[0m sarl\n",
            "\u001b[90m2023-08-30 06:05:04 \u001b[96m1/192 Synthesizing\u001b[0m bishop ay 1....\n",
            "Generating autoregressive samples..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b46a05a39fed>\u001b[0m in \u001b[0;36m<cell line: 130>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_txt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mappend_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Out: '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_out\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mvoice_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconditioning_latents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_voice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtts_with_preset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoice_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvoice_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconditioning_latents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconditioning_latents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/tortoise-tts/tortoise/api.py\u001b[0m in \u001b[0;36mtts_with_preset\u001b[0;34m(self, text, preset, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpresets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# allow overriding of preset settings with kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     def tts(self, text, voice_samples=None, conditioning_latents=None, k=1, verbose=True, use_deterministic_seed=None,\n",
            "\u001b[0;32m/content/tortoise-tts/tortoise/api.py\u001b[0m in \u001b[0;36mtts\u001b[0;34m(self, text, voice_samples, conditioning_latents, k, verbose, use_deterministic_seed, return_deterministic_state, num_autoregressive_samples, temperature, length_penalty, repetition_penalty, top_p, max_mel_tokens, cvvp_amount, diffusion_iterations, cond_free, cond_free_k, diffusion_temperature, **hf_generate_kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 ) as autoregressive, torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=self.half):\n\u001b[1;32m    429\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                         codes = autoregressive.inference_speech(auto_conditioning, text_tokens,\n\u001b[0m\u001b[1;32m    431\u001b[0m                                                                     \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                                                                     \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/tortoise-tts/tortoise/models/autoregressive.py\u001b[0m in \u001b[0;36minference_speech\u001b[0;34m(self, speech_conditioning_latent, text_inputs, input_tokens, num_return_sequences, max_generate_length, typical_sampling, typical_mass, **hf_generate_kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mlogits_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogitsProcessorList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTypicalLogitsWarper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtypical_mass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtypical_sampling\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mLogitsProcessorList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrunc_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_mel_tokens\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;32mif\u001b[0m \u001b[0mmax_generate_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtrunc_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax_generate_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         gen = self.inference_model.generate(inputs, bos_token_id=self.start_mel_token, pad_token_id=self.stop_mel_token, eos_token_id=self.stop_mel_token,\n\u001b[0m\u001b[1;32m    539\u001b[0m                                             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m                                             num_return_sequences=num_return_sequences, **hf_generate_kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1588\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1589\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2641\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2642\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2643\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2644\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/tortoise-tts/tortoise/models/autoregressive.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmel_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             )\n\u001b[0;32m--> 149\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    898\u001b[0m                 )\n\u001b[1;32m    899\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    901\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}